Copyright (c) 2015 Digi International Inc.

This Source Code Form is subject to the terms of the Mozilla Public
License, v. 2.0. If a copy of the MPL was not distributed with this
file, You can obtain one at http://mozilla.org/MPL/2.0/.

-------------------------------------------------------------------------
Rabbit Semiconductor Rabbit Floating Point Package (ZFPP)
-------------------------------------------------------------------------

Floating point format (compatible with Intel/ IEE, etc.) is as follows:

    seeeeeee emmmmmmm mmmmmmmm mmmmmmmm (s-sign, e-exponent, m-mantissa)

stored in memory with the 4 bytes reversed from shown above.
s- 1 negative, 0- positive
e - 0-255 indicating the exponent
m- mantissa 23 bits with implied 24th bit which is always 1
exponent is biased by the amount bias set below
;
mantissa, when the hidden bit is added in, is 24 bits long
and has a value in the range of 1.000 to 1.9999..
;
to match Intel 8087 or IEEE format use bias of 127
;
examples of numbers:
  sign  exponent   mantissa
    0   01111110 (1) 10000....    1.5 * 2 ^ (-1) = 0.75
    0   01111111 (1) 10000....    1.5 * 2 ^ ( 0 )= 1.50
    1   10000000 (1) 10000....   -1.5 * 2 ^ ( 1 )= -3.00
    0   10000110 (1) 01100100010..         =178.25
    x   00000000     000....         zero (plus or minus)
    0   11111111 (1) 000... positive infinity
    1   11111111 (1) 000... negative infinity

Calling Sequences for floating point numbers

example X=Y*Z;
    ld    de,(Y)   least part
    ld    bc,(Y+2) most part
    push  bc
    push  de
    ld    de,(Z)
    ld    bc,(Z+2)
    call  F_mul
    ld    (X),de
    ld    (X+2),bc

example Z=Y/Z;
    ld    de,(Y)
    ld    bc,(Y+2)
    push  bc
    push  de
    ld    de,(Z)
    ld    bc,(Z+2)
    call  F_div
    ld    (X),de
    ld    (X+2),bc

example Z=Y+Z;
    ld    de,(Y)
    ld    bc,(Y+2)
    push  bc
    push  de
    ld    de,(Z)
    ld    bc,(Z+2)
    call  F_add
    ld    (X),de
    ld    (X+2),bc

example Z=Y-Z;
    ld    de,(Y)
    ld    bc,(Y+2)
    push  bc
    push  de
    ld    de,(Z)
    ld    bc,(Z+2)
    call  F_sub
    ld    (X),de
    ld    (X+2),bc

example  X=-Z;
    ld    de,(Z)
    ld    bc,(Z+2)
     call  F_neg
    ld    (X),de
    ld    (X+2),bc

The Rabbit Semiconductor floating point package is loosely based on IEEE 754. We
maintain the packed format, but we do not support denormal numbers or
the round to even convention.  Both of these features could be added
in the future with some performance penalty.

IEEE floating point format: 	seeeeeee emmmmmmm mmmmmmmm mmmmmmmm

represents  e>0             -> (-1)^s * 2^e * (0x800000 + m)/0x800000
            e=0             -> (-1)^s * 2^e * m/0x800000
            e=0xff & m=0    -> (-1)^s * INF
            e=0xff & m!=0   -> (-1)^s NAN

Where s is the sign, e is the exponent and m is bits 22-0 of the
mantissa. ZFPP assumes any number with a zero exponent is zero.
IEEE/ZFPP assume bit 23 of the mantissa is 1 except where the exponent
is zero.

IEEE specifies rounding the result by a process of round to even.  IEEE
uses one guard bit and a sticky bit to round a result per the following
table

-------------------------------------------------------------------------
IEEE round to nearest:
b g s  (b=lsbit g=guard s=sticky)
0 0 0  exact
0 0 1  -.001
0 1 0  -.01
0 1 1  +.001
1 0 0	exact
1 0 1  -.001
1 1 0  +.01
1 1 1  +.001

-------------------------------------------------------------------------
ZFPP rounds the number using a single sticky bit which is ored to
with the lsb of the result:
b s
0 0		exact
0 1		+.01
1 0		exact
1 1		-.01

-------------------------------------------------------------------------
Both results are free of bias with IEEE method having a slight edge with
rounding error.
-------------------------------------------------------------------------


******** Math Library Discussion *******

Functions included in math libaries

Basic floating point functions - these are computed from first principles

 F_sub, F_add, F_mul, F_div, F_neg - add, subtrace, multiply, divide, negate (in multifp.lib)
 deg(x) - degrees in x radians
 rad(x) - radians in x degrees

Derrivitive floating point functions (derived from combinations of basic functions)

 pow10(x)- 10 to the x power
 acsc(x) - arc cosecant of x
 asec(x) - arc secant of x
 acot(x) - arc cotangent of x

Discussion of Accuracy

 Generally the basic functions are accurate within 1-3 counts of the floating mantissa. However, in
 certain ranges of certain functions the relative accuracy is much less do to the intrinsic properties of
 floating point math. Accuracy expressed in counts of the floating mantissa is relative acccuracy -
 i.e. relative to the size of the number. Absolute accuracy is the absolute size of the error - e.g.
 .00001. The derivative functions, computed as combinations of the basic functions, typically
 have larger error because the errors of 2 or more basic functions are added together in some fashion.

 If the value of the function depends on the value of the difference of 2 floating point numbers that are
 close to each other in value, the relative error generally becomes large, although the absolute error
 may remain well bounded. Examples are the logs of numbers near 1 and the sine of numbers near pi.
 For example, if the argument of the sine function is a floating point number is close to pi, say
 5 counts of the mantissa away from pi and it is subtracted from pi the result will be a number with
 only 3 significent bits. The relative error in the sine result will be very large, but the absolute
 error will still be very small. Functions with steep slopes, such as the exponent of larger numbers
 will show a large relative error, since the relative error in the argument is magnified by the slope.

Basic floating point functions - these are computed from first principles

 pow2(x) - 2 to the power x
 log2(x) - logarithm of x base 2

 atan(x) - arc tangent
 sin(x) - sine
 sqrt(x) - square root
 ceil(x) - smallest integer greater than or equal to x
 fabs(x) - absolute value x
 floor(x) - largest integer less than or equal to x
 fmod(x,&n) - integer and fractional parts

Derivitive floating point functions (derived from combinations of basic functions)

 acos(x) - arc cosine of x
 asin(x) - arc sine of x
 atan2(y,x) - arctan of y/x
 cos(x) - cosine of x
 tan(x)- tangent of x
 cosh(x) - hyperbolic cosine of x
 sinh(x) - hyperbolic sine of x
 tanh(x) - hyperbolic tangent of x
 exp(x) - e to the x power
 log(x) - logarithm of x base e
 log10(x) - logarithm of x base 10
 pow(x,y) - x to y power

Discussion of Accuracy

 Generally the basic functions are accurate within 1-3 counts of the floating mantissa. However, in
 certain ranges of certain functions the relative accuracy is much less do to the intrinsic properties of
 floating point math. Accuracy expressed in counts of the floating mantissa is relative acccuracy -
 i.e. relative to the size of the number. Absolute accuracy is the absolute size of the error - e.g.
 .00001. The derivative functions, computed as combinations of the basic functions, typically
 have larger error because the errors of 2 or more basic functions are added together in some fashion.

 If the value of the function depends on the value of the difference of 2 floating point numbers that are
 close to each other in value, the relative error generally becomes large, although the absolute error
 may remain well bounded. Examples are the logs of numbers near 1 and the sine of numbers near pi.
 For example, if the argument of the sine function is a floating point number is close to pi, say
 5 counts of the mantissa away from pi and it is subtracted from pi the result will be a number with
 only 3 significent bits. The relative error in the sine result will be very large, but the absolute
 error will still be very small. Functions with steep slopes, such as the exponent of larger numbers
 will show a large relative error, since the relative error in the argument is magnified by the slope.

Discussion of execution speed

 Floating add, subtract and multiply require approximately 350 clocks worst case on the Rabbit 2000
 microprocessor. Divide and square root require approximately 900 clocks. Sine and pow2, pow10 or exp
 require about 3200 clocks. Log, log2, log (base e), and atan need about 4000 clocks. Functions
 derrived from these functions often require 5000 or more clocks.

Exceptions - range errors

 Certain values will result in an exception. If debugging is in process this will result in an error message.
 If the exception takes place in a program in the field the response is entry into the error log
 (planned) and a watchdog timeout. Exceptions occur for:

 1) Square root of a negative number
 2) argument of exponent type function too large (x>129.9).
 3) Log of a negative number.

